{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to The RaBitQ Library","text":"<p>The RaBitQ Library provides efficient and lightweight implementations of the RaBitQ quantization algorithm (1-bit version and multi-bit version) and its applications in high-dimensional vector search. The core algorithm RaBitQ is based on the research from VectorDB group at Nanyang Technological University, Singapore. </p> <p>The library provides the following key features:</p> <ul> <li>RaBitQ - a vector quantization algorithm as a drop-in replacement of binary and scalar quantization, offering an optimal theoretical error bound</li> <li>RaBitQ for Vector Search - a reference implementation of RaBitQ's combination with popular vector search indexes</li> </ul> <p>The RaBitQ Library supports estimating similarity metrics including Euclidean distance, inner product and cosine similarity.</p>"},{"location":"#rabitq","title":"RaBitQ","text":"<p>RaBitQ is a vector quantization algorithm as a drop-in replacement of binary and scalar quantization. The key advantages of RaBitQ include</p> <ul> <li>High Accuracy with Tiny Space - RaBitQ achieves the state-of-the-art accuracy under diverse bit-width for the estimation of similarity metrics. It produces promising accuracy with even 1-bit per dimension.</li> <li>Fast Distance Estimation - RaBitQ supports to estimate the similarity metrics with high efficiency based on bitwise operations or FastScan.</li> <li>Theoretical Error Bound - RaBitQ provides an asymptotically optimal error bound for the estimation of distances and inner product. The error bound can be used for reliable ordering and reranking.</li> </ul> <p>In this library, we provide simple interfaces to support advanced features of RaBitQ. The details are presented in RaBitQ.</p>"},{"location":"#rabitq-for-vector-search","title":"RaBitQ for Vector Search","text":"<p>In the library, RaBitQ is combined with IVF, HNSW and QG to deliever different trade-offs among time, space and accuracy. </p> <p>Using RaBitQ with IVF and HNSW targets a balance between memory consumption and query performance. Only the quantization codes produced by RaBitQ are stored and the raw data vectors are not accessed during querying. Thus, these methods consume less memory than the raw dataset.  Using 4-bit, 5-bit and 7-bit quantization usually suffices to produce 90%, 95% and 99% recall respectively without reranking. </p> <p>Using RaBitQ with QG targets the best query performance by using more memory. It creates multiple quantization codes for every vector to optimize the data access pattern. Thus, QG usually consumes 2x memory of the raw dataset. </p>"},{"location":"#rabitq-in-industry","title":"RaBitQ in Industry","text":"<p>The RaBitQ algorithm has been implemented in many real-world systems in industry including </p> <ul> <li>Milvus - IVF + RaBitQ (C++)</li> <li>Faiss - IVF + RaBitQ (C++)</li> <li>VSAG - HGraph + RaBitQ (C++)</li> <li>VectorChord - IVF + RaBitQ (Rust)</li> <li>Volcengine OpenSearch - DiskANN + RaBitQ</li> <li>CockroachDB - CSPANN + RaBitQ (Golang)</li> <li>ElasticSearch - HNSW + RaBitQ (Java - the algorithm is adopted with some minor modifications and renamed as \"BBQ\")</li> <li>Lucene - HNSW + RaBitQ (Java - the algorithm is adopted with some minor modifications and renamed as \"BBQ\")</li> </ul>"},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>We acknowledge Alexandr Guzhva, Li Liu, Chao Gao, Silu Huang, Jiabao Jin, Xiaoyao Zhong and Jinjing Zhou for valuable feedbacks. </p>"},{"location":"#reference","title":"Reference","text":"<p>Please provide a reference of our paper if it helps in your systems or research projects.</p> <pre>\nJianyang Gao, Yutong Gou, Yuexuan Xu, Yongyi Yang, Cheng Long, Raymond Chi-Wing Wong, \"Practical and Asymptotically Optimal Quantization of High-Dimensional Vectors in Euclidean Space for Approximate Nearest Neighbor Search\", SIGMOD 2025, available at https://arxiv.org/abs/2409.09913\n</pre>"},{"location":"compact_code/","title":"Compact Storage of Codes","text":"<p>This section describes the compact storage of codes. RaBitQLib supports to quantize codes with different bit widths, i.e., 1, 2, 3, 4, 5, 6, 7 and 8. These bit widths except 8 are unaligned with byte alignment. Thus, we need to design a specialized compact storage format for the code vector for each bit width. We pad the dimensionality to a multiple of 64 for the ease of alignment. The implementation can be found in <code>rabitqlib/quantization/pack_excode.hpp</code>.</p> <p>Example  <pre><code>#include &lt;rabitqlib/quantization/pack_excode.hpp&gt;\n#include &lt;stdint.h&gt;\n#include &lt;random&gt;\n\nint main(){\n    size_t dim = 768;\n    size_t bits = 4;\n\n    std::vector&lt;uint8_t&gt; code(dim);\n    // Generate random 4-bit values (0-15) for each dimension\n    for (size_t i = 0; i &lt; dim; ++i) {\n        code[i] = rand() % 16;  // 4-bit values range from 0 to 15\n    }\n\n    std::vector&lt;uint8_t&gt; compact_code(dim * bits / 8);\n\n    rabitqlib::quant::rabitq_impl::ex_bits::packing_rabitqplus_code(\n        code.data(), compact_code.data(), dim, bits\n    );\n}\n</code></pre></p> <p>The following shows the compact storage format for every 64 dimensions.</p>"},{"location":"compact_code/#1-bit","title":"1-bit","text":"<p>The code sequentially stores the binary value for each of the 64 dimensions. </p>"},{"location":"compact_code/#2-bit","title":"2-bit","text":"<p>The code is stored in a byte array of length 16. Each row in the figure represents a byte. The 0-th byte stores the 2-bit codes of the 0-th, 16-th, 32-th and 48-th dimensions. The 1-th byte stores the 2-bit codes of the 1-th, 17-th, 33-th and 49-th dimensions, so on and so forth. This storage allows efficient unpacking with SIMD, i.e., shifting and masking with <code>SSE</code>. </p>"},{"location":"compact_code/#3-bit-2-bit-1-bit","title":"3-bit = 2-bit + 1-bit","text":""},{"location":"compact_code/#4-bit","title":"4-bit","text":"<p>The code is stored in a byte array of length 32. The 0-th byte stores the 4-bit codes of the 0-th and 16-th dimensions. The 1-th byte stores the 4-bit codes of the 1-th and 17-th dimensions, so on and so forth. This storage allows efficient unpacking with SIMD, i.e., shifting and masking with <code>SSE</code>. </p>"},{"location":"compact_code/#5-bit-4-bit-1-bit","title":"5-bit = 4-bit + 1-bit","text":""},{"location":"compact_code/#6-bit","title":"6-bit","text":"<p>The code is stored in a byte array of length 48. </p> <ul> <li>The first 16 bytes store the 6-bit codes of the 0-th to the 15-th dimensions and the upper 2-bit codes of the 32-th to 47-th dimensions.</li> <li>The second 16 bytes store the 6-bit codes of the 16-th to the 31-th dimensions and the upper 2-bit codes of the 48-th to 63-th dimensions.</li> <li>The third 16 bytes store the lower 4-bit codes of the 32-th to 47-th dimensions and the lower 4-bit codes of the 48-th to 63-th dimensions.</li> </ul> <p>This storage allows efficient unpacking with SIMD, i.e., shifting and masking with <code>SSE</code>. </p>"},{"location":"compact_code/#7-bit-6-bit-1-bit","title":"7-bit = 6-bit + 1-bit","text":""},{"location":"compact_code/#8-bit","title":"8-bit","text":"<p>The code of 8-bit is aligned with byte arrays and needs no specialized design.</p>"},{"location":"kernel_ip/","title":"Kernel - Inner Product","text":"<p>The implementation of computing inner product between binary codes and query vectors heavily affects the efficiency. The best implementation may vary largely across platforms and the dimensionality of datasets. For now, we only include the implementation with <code>__builtin_popcountll</code> and expect that compilers will automatically vectorize it.</p> <p>This part introduces how to compute the inner product between quantization codes and rotated query vectors i.e., \\left&lt; \\mathbf{x}_0,\\mathbf{q}_r'\\right&gt; and \\left&lt; \\mathbf{x}_u,\\mathbf{q}_r'\\right&gt;. The implementation includes two types:</p> <ol> <li>The inner product between binary codes and \\mathbf{q}_r'.</li> <li>The inner product between multi-bit codes and \\mathbf{q}_r'.</li> </ol>"},{"location":"kernel_ip/#the-kernel-for-binary-codes","title":"The Kernel for Binary Codes","text":""},{"location":"kernel_ip/#single-code","title":"Single Code","text":"<p>We compute the inner product between a single binary vector \\mathbf{x}_0 and a floating-point vector \\mathbf{q}_r' via bitwise-and <code>&amp;</code> and <code>popcnt</code>. We first quantize \\mathbf{q}_r' into a vector of 4-bit unsigned integers (based on a fast version of RaBitQ). </p>  \\mathbf{q}_r'\\approx \\Delta_q \\mathbf{q}_u + v_q\\cdot \\mathbf{1}_D  <p>Let \\mathbf{q}_u^{(i)} be the i-th bit of \\mathbf{q}_u. Then with the quantized vector \\mathbf{q}_u, we can compute the inner product based on the following formula:</p>  \\begin{align} \\left&lt; \\mathbf{x}_0,\\mathbf{q}_r'\\right&gt; &amp;\\approx \\left&lt; \\mathbf{x}_0,\\Delta_q \\mathbf{q}_u + v_q\\cdot \\mathbf{1}_D\\right&gt; \\\\&amp;=\\Delta_q \\left&lt; \\mathbf{x}_0,\\mathbf{q}_u\\right&gt; + v_q \\cdot \\mathrm{popcnt}(\\mathbf{x}_0) \\\\ &amp;=\\Delta_q \\sum_{i=0}^{B_q-1} \\left( 2^i\\left&lt; \\mathbf{x}_0,\\mathbf{q}_u^{(i)}\\right&gt; \\right) + v_q \\cdot\\mathrm{popcnt}(\\mathbf{x}_0) \\\\ &amp;=\\Delta_q \\sum_{i=0}^{B_q-1} 2^i\\cdot \\mathrm{popcnt}(\\mathbf{x}_0\\ \\&amp;\\  \\mathbf{q}_u^{(i)})  + v_q \\cdot \\mathrm{popcnt}(\\mathbf{x}_0) \\end{align}"},{"location":"kernel_ip/#batch-code","title":"Batch Code","text":"<p>We compute the inner product between a batch of binary vectors \\mathbf{x}_0 and a floating-point vector \\mathbf{q}_r' via <code>FastScan</code>. Here we provide a brief introduction and refer readers to a detailed tutorial by Faiss.</p> <p>For a D-bit binary, we split it into M=D/4 segments. We prepare look-up-tables for each segment. - \\mathrm{LUT}[m][mask]: the inner product with \\mathbf{q}_r' in the m-th segment, i.e., the (4m)-th to the (4m+3)-th dimensions, when the code in the m-th segment equals to mask. </p> <p>Based on the look-up-tables, we can compute the inner product as follows:</p>  \\begin{align} \\left&lt; \\mathbf{x}_0,\\mathbf{q}_r'\\right&gt;= \\sum_{m=0}^{M-1}  \\mathrm{LUT}[m][\\mathbf{x}_{0}[4m:4m+3]]  \\end{align}"},{"location":"kernel_ip/#the-kernel-for-multi-bit-codes","title":"The Kernel for Multi-bit Codes","text":"<p>For the multi-bit codes, we convert the unsigned integer codes to floating point numbers with native instructions of AVX512. </p>"},{"location":"quick_start/","title":"Quick Start","text":""},{"location":"quick_start/#rabitq-quantizer","title":"RaBitQ Quantizer","text":"<p>The RaBitQ Library offers simple interfaces, making it a drop-in replacement for scalar and binary quantization.  The interface offers two underlying implementations of RaBitQ: one delivers optimal accuracy with longer quantization time, while the other provides near-optimal accuracy with significantly faster quantization.</p> <p>The library provides advanced data formats for supporting efficient distance estimation. The details can be found in Quantizer.</p>"},{"location":"quick_start/#example-code-in-c","title":"Example Code in C++","text":"<pre><code>#include &lt;random&gt;\n#include &lt;vector&gt;\n\n#include \"quantization/rabitq.hpp\"\n\nint main() {\n    size_t dim = 128;\n\n    std::vector&lt;float&gt; vector(dim);\n    static std::random_device rd;\n    static std::mt19937 gen(rd());\n    std::normal_distribution&lt;float&gt; dist(0, 1);\n\n    // generate a random vector\n    for (size_t i = 0; i &lt; dim; ++i) {\n        vector[i] = dist(gen);\n    }\n\n    size_t bits = 8;                  // num of bits for total code\n    std::vector&lt;uint32_t&gt; code(dim);  // code\n    float delta;                      // delta for scalar quantization\n    float vl;                         // lower value for scalar quantization\n\n    // scalar quantization\n    rabitqlib::quant::quantize_scalar(vector.data(), dim, bits, code.data(), delta, vl);\n\n    // faster version, must init a config struct first\n    rabitqlib::quant::RabitqConfig config = rabitqlib::quant::faster_config(dim, bits);\n    rabitqlib::quant::quantize_scalar(\n        vector.data(), dim, bits, code.data(), delta, vl, config\n    );\n\n    // reconstruct  \n    float * reconstructed_data = new float [padded_dim];\n    rabitqlib::quant::reconstruct_vec(code, delta, vl, padded_dim, reconstructed_data);\n\n    return 0;\n}\n</code></pre>"},{"location":"quick_start/#rabitq-ivf","title":"RaBitQ + IVF","text":"<p>IVF is a classical clustering-based ANN index. IVF + RaBitQ consumes minimum memory across IVF, HNSW and QG. Powered by FastScan, it also achieves promising time-accuracy trade-off. To use RaBitQ + IVF, users need to cluster raw data vectors (e.g., using Kmeans), then to quantize each cluster and construct the IVF. The following is an example of using RaBitQ + IVF to search ANN on the deep1M dataset. </p>"},{"location":"quick_start/#dataset-downloading-and-clustering","title":"Dataset downloading and clustering","text":"<p>Use the following commands in the shell to download the deep1M dataset, generate clustering information, and store it on disk. <pre><code>wget http://www.cse.cuhk.edu.hk/systems/hash/gqr/dataset/deep1M.tar.gz\ntar -zxvf deep1M.tar.gz\npython python/ivf.py deep1M/deep1M_base.fvecs 4096 deep1M/deep1M_centroids_4096.fvecs deep1M/deep1M_clusterids_4096.ivecs\n</code></pre></p>"},{"location":"quick_start/#example-code-in-c-for-index-construction","title":"Example Code in C++ for index construction","text":"<p>The following codes show how to load deep1M's vector data, centroids information, and cluster ids from disk, build the IVF + RaBitQ index,  and finally save the index to disk. <pre><code>#include &lt;cstdint&gt;\n#include &lt;iostream&gt;\n\n#include \"defines.hpp\"\n#include \"index/ivf/ivf.hpp\"\n#include \"utils/io.hpp\"\n#include \"utils/stopw.hpp\"\n\nusing PID = rabitqlib::PID;\nusing index_type = rabitqlib::ivf::IVF;\nusing data_type = rabitqlib::RowMajorArray&lt;float&gt;;\nusing gt_type = rabitqlib::RowMajorArray&lt;uint32_t&gt;;\n\nint main(int argc, char** argv) {\n    if (argc &lt; 6) {\n        std::cerr &lt;&lt; \"Usage: \" &lt;&lt; argv[0] &lt;&lt; \" &lt;arg1&gt; &lt;arg2&gt; &lt;arg3&gt; &lt;arg4&gt; &lt;arg5&gt;\\n\"\n                  &lt;&lt; \"arg1: path for data file, format .fvecs\\n\"\n                  &lt;&lt; \"arg2: path for centroids file generated by ivf.py\\n\"\n                  &lt;&lt; \"arg3: path for cluster ids file generated by ivf.py\\n\"\n                  &lt;&lt; \"arg4: total number of bits for quantization\\n\"\n                  &lt;&lt; \"arg5: path for saving index\\n\"\n                  &lt;&lt; \"arg6: if use faster quantization (\\\"true\\\" or \\\"false\\\"), false by \"\n                     \"default\\n\";\n        exit(1);\n    }\n\n    bool faster_quant = false;\n    if (argc &gt; 6) {\n        std::string faster_str(argv[6]);\n        if (faster_str == \"true\") {\n            faster_quant = true;\n            std::cout &lt;&lt; \"Using faster quantize for indexing...\\n\";\n        }\n    }\n\n    char* data_file = argv[1];\n    char* centroids_file = argv[2];\n    char* cids_file = argv[3];\n    size_t total_bits = atoi(argv[4]);\n    char* index_file = argv[5];\n\n    data_type data;\n    data_type centroids;\n    gt_type cids;\n\n    rabitqlib::load_vecs&lt;float, data_type&gt;(data_file, data);\n    rabitqlib::load_vecs&lt;float, data_type&gt;(centroids_file, centroids);\n    rabitqlib::load_vecs&lt;PID, gt_type&gt;(cids_file, cids);\n\n    size_t num_points = data.rows();\n    size_t dim = data.cols();\n    size_t k = centroids.rows();\n\n    std::cout &lt;&lt; \"data loaded\\n\";\n    std::cout &lt;&lt; \"\\tN: \" &lt;&lt; num_points &lt;&lt; '\\n';\n    std::cout &lt;&lt; \"\\tDIM: \" &lt;&lt; dim &lt;&lt; '\\n';\n\n    rabitqlib::StopW stopw;\n    index_type ivf(num_points, dim, k, total_bits);\n    ivf.construct(data.data(), centroids.data(), cids.data(), faster_quant);\n    float miniutes = stopw.get_elapsed_mili() / 1000 / 60;\n    std::cout &lt;&lt; \"ivf constructed \\n\";\n    ivf.save(index_file);\n\n    std::cout &lt;&lt; \"Indexing time \" &lt;&lt; miniutes &lt;&lt; '\\n';\n\n    return 0;\n}\n</code></pre> After compilation (suppose it is compiled to an executable named <code>ivf_build</code>), run the following command to build the IVF: <pre><code>./ivf_build deep1M/deep1M_base.fvecs deep1M/deep1M_centroids_4096.fvecs deep1M/deep1M_clusterids_4096.ivecs 4 deep1M/deep1M_rabitqlib_ivf_4.index true\n</code></pre> This will build an IVF that uses 4 (1+3) bits to quantize each vector for the deep1M dataset through RaBitQ.</p>"},{"location":"quick_start/#example-code-in-c-for-querying","title":"Example Code in C++ for querying","text":"<p>After building the index, you can execute queries on it. The following codes show how to load ivf index and query from disk, execute queries, and compare the results to the groundtruth.</p> <p><pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\n#include \"defines.hpp\"\n#include \"index/ivf/ivf.hpp\"\n#include \"utils/io.hpp\"\n#include \"utils/stopw.hpp\"\n#include \"utils/tools.hpp\"\n\nusing PID = rabitqlib::PID;\nusing index_type = rabitqlib::ivf::IVF;\nusing data_type = rabitqlib::RowMajorArray&lt;float&gt;;\nusing gt_type = rabitqlib::RowMajorArray&lt;uint32_t&gt;;\n\nstatic std::vector&lt;size_t&gt; get_nprobes(\n    const index_type&amp; ivf,\n    const std::vector&lt;size_t&gt;&amp; all_nprobes,\n    data_type&amp; query,\n    gt_type&amp; gt\n);\n\nstatic size_t topk = 10;\nstatic size_t test_round = 5;\n\nint main(int argc, char** argv) {\n    if (argc &lt; 4) {\n        std::cerr &lt;&lt; \"Usage: \" &lt;&lt; argv[0] &lt;&lt; \" &lt;arg1&gt; &lt;arg2&gt; &lt;arg3&gt; &lt;arg4&gt;\\n\"\n                  &lt;&lt; \"arg1: path for index \\n\"\n                  &lt;&lt; \"arg2: path for query file, format .fvecs\\n\"\n                  &lt;&lt; \"arg3: path for groundtruth file format .ivecs\\n\"\n                  &lt;&lt; \"arg4: whether use high accuracy fastscan, (\\\"true\\\" or \\\"false\\\"), \"\n                     \"true by default\\n\\n\";\n        exit(1);\n    }\n\n    char* index_file = argv[1];\n    char* query_file = argv[2];\n    char* gt_file = argv[3];\n    bool use_hacc = true;\n\n    if (argc &gt; 4) {\n        std::string hacc_str(argv[4]);\n        if (hacc_str == \"false\") {\n            use_hacc = false;\n            std::cout &lt;&lt; \"Do not use Hacc FastScan\\n\";\n        }\n    }\n\n    data_type query;\n    gt_type gt;\n    rabitqlib::load_vecs&lt;float, data_type&gt;(query_file, query);\n    rabitqlib::load_vecs&lt;uint32_t, gt_type&gt;(gt_file, gt);\n    size_t nq = query.rows();\n    size_t total_count = nq * topk;\n\n    index_type ivf;\n    ivf.load(index_file);\n\n    std::vector&lt;size_t&gt; all_nprobes;\n    all_nprobes.push_back(5);\n    for (size_t i = 10; i &lt; 200; i += 10) {\n        all_nprobes.push_back(i);\n    }\n    for (size_t i = 200; i &lt; 400; i += 40) {\n        all_nprobes.push_back(i);\n    }\n    for (size_t i = 400; i &lt;= 1500; i += 100) {\n        all_nprobes.push_back(i);\n    }\n    for (size_t i = 2000; i &lt;= 4000; i += 500) {\n        all_nprobes.push_back(i);\n    }\n\n    all_nprobes.push_back(6000);\n    all_nprobes.push_back(10000);\n    all_nprobes.push_back(15000);\n\n    rabitqlib::StopW stopw;\n\n    auto nprobes = get_nprobes(ivf, all_nprobes, query, gt);\n    size_t length = nprobes.size();\n\n    std::vector&lt;std::vector&lt;float&gt;&gt; all_qps(test_round, std::vector&lt;float&gt;(length));\n    std::vector&lt;std::vector&lt;float&gt;&gt; all_recall(test_round, std::vector&lt;float&gt;(length));\n\n    for (size_t r = 0; r &lt; test_round; r++) {\n        for (size_t l = 0; l &lt; length; ++l) {\n            size_t nprobe = nprobes[l];\n            size_t total_correct = 0;\n            float total_time = 0;\n            std::vector&lt;PID&gt; results(topk);\n            for (size_t i = 0; i &lt; nq; i++) {\n                stopw.reset();\n                ivf.search(&amp;query(i, 0), topk, nprobe, results.data(), use_hacc);\n                total_time += stopw.get_elapsed_micro();\n                for (size_t j = 0; j &lt; topk; j++) {\n                    for (size_t k = 0; k &lt; topk; k++) {\n                        if (gt(i, k) == results[j]) {\n                            total_correct++;\n                            break;\n                        }\n                    }\n                }\n            }\n            float qps = static_cast&lt;float&gt;(nq) / (total_time / 1e6F);\n            float recall =\n                static_cast&lt;float&gt;(total_correct) / static_cast&lt;float&gt;(total_count);\n\n            all_qps[r][l] = qps;\n            all_recall[r][l] = recall;\n        }\n    }\n\n    auto avg_qps = rabitqlib::horizontal_avg(all_qps);\n    auto avg_recall = rabitqlib::horizontal_avg(all_recall);\n\n    std::cout &lt;&lt; \"nprobe\\tQPS\\trecall\" &lt;&lt; '\\n';\n\n    for (size_t i = 0; i &lt; length; ++i) {\n        size_t nprobe = nprobes[i];\n        float qps = avg_qps[i];\n        float recall = avg_recall[i];\n\n        std::cout &lt;&lt; nprobe &lt;&lt; '\\t' &lt;&lt; qps &lt;&lt; '\\t' &lt;&lt; recall &lt;&lt; '\\n';\n    }\n\n    return 0;\n}\n\nstatic std::vector&lt;size_t&gt; get_nprobes(\n    const index_type&amp; ivf,\n    const std::vector&lt;size_t&gt;&amp; all_nprobes,\n    data_type&amp; query,\n    gt_type&amp; gt\n) {\n    size_t nq = query.rows();\n    size_t total_count = topk * nq;\n    float old_recall = 0;\n    std::vector&lt;size_t&gt; nprobes;\n\n    for (auto nprobe : all_nprobes) {\n        nprobes.push_back(nprobe);\n\n        size_t total_correct = 0;\n        std::vector&lt;PID&gt; results(topk);\n        for (size_t i = 0; i &lt; nq; i++) {\n            ivf.search(&amp;query(i, 0), topk, nprobe, results.data());\n            for (size_t j = 0; j &lt; topk; j++) {\n                for (size_t k = 0; k &lt; topk; k++) {\n                    if (gt(i, k) == results[j]) {\n                        total_correct++;\n                        break;\n                    }\n                }\n            }\n        }\n        float recall = static_cast&lt;float&gt;(total_correct) / static_cast&lt;float&gt;(total_count);\n        if (recall &gt; 0.997 || recall - old_recall &lt; 1e-5) {\n            break;\n        }\n        old_recall = recall;\n    }\n\n    return nprobes;\n}\n</code></pre> To execute queries on deep1M, run the following command for the compiled codes (suppose that it is named <code>ivf_query</code>): <pre><code>./ivf_query deep1M/deep1M_rabitqlib_ivf_4.index deep1M/deep1M_query.fvecs deep1M/deep1M_groundtruth.ivecs\n</code></pre></p>"},{"location":"quick_start/#rabitq-hnsw","title":"RaBitQ + HNSW","text":"<p>HNSW is a popular graph-based index. HNSW + RaBitQ consumes the more memory than IVF + RaBitQ because it needs to store the edges of every vertex in a graph (e.g., 32 edges = 1,024 bits). In terms of the time-accuracy trade-off, HNSW + RaBitQ and IVF + RaBitQ perform differently across datasets\u2014sometimes the former works better, and sometimes the latter does. RaBitQ + HNSW receives raw data vectors as inputs. It first conducts KMeans using a Python script. The centroid vectors will be used in the normalization of data vectors for improving accuracy. </p>"},{"location":"quick_start/#perform-clustering-using-faiss","title":"Perform Clustering using Faiss","text":"<p>First, conduct Kmeans clustering on raw data vectors to get centroid vectors. We recommend 16 centroids(clusters). This will produce two files: centroids file and cluster ids file.</p>"},{"location":"quick_start/#example-code-in-c-for-index-construction_1","title":"Example Code in C++ for index construction","text":"<p>Second, load raw data, centroids, and cluster ids files to build the index. Index file is then saved.</p> <pre><code>#include &lt;cstdint&gt;\n#include &lt;iostream&gt;\n\n#include \"index/hnsw/hnsw.hpp\"\n#include \"utils/io.hpp\"\n#include \"utils/stopw.hpp\"\n\nusing PID = rabitqlib::PID;\nusing index_type = rabitqlib::hnsw::HierarchicalNSW;\nusing data_type = rabitqlib::RowMajorArray&lt;float&gt;;\nusing gt_type = rabitqlib::RowMajorArray&lt;uint32_t&gt;;\n\nint main(int argc, char* argv[]) {\n    if (argc &lt; 8) {\n        std::cerr &lt;&lt; \"Usage: \" &lt;&lt; argv[0]\n                  &lt;&lt; \" &lt;arg1&gt; &lt;arg2&gt; &lt;arg3&gt; &lt;arg4&gt; &lt;arg5&gt; &lt;arg6&gt; &lt;arg7&gt; &lt;arg8&gt;\\n\"\n                  &lt;&lt; \"arg1: path for data file, format .fvecs\\n\"\n                  &lt;&lt; \"arg2: path for centroids file, format .fvecs\\n\"\n                  &lt;&lt; \"arg3: path for cluster ids file, format .ivecs\\n\"\n                  &lt;&lt; \"arg4: m (degree bound) for hnsw\\n\"\n                  &lt;&lt; \"arg5: ef for indexing \\n\"\n                  &lt;&lt; \"arg6: total number of bits for quantization\\n\"\n                  &lt;&lt; \"arg7: path for saving index\\n\"\n                  &lt;&lt; \"arg8: metric type (\\\"l2\\\" or \\\"ip\\\")\\n\"\n                  &lt;&lt; \"arg9: if use faster quantization (\\\"true\\\" or \\\"false\\\"), false by \"\n                     \"default\\n\";\n        exit(1);\n    }\n\n    char* data_file = argv[1];\n    char* centroid_file = argv[2];\n    char* cid_file = argv[3];\n    size_t m = atoi(argv[4]);\n    size_t ef = atoi(argv[5]);\n    size_t total_bits = atoi(argv[6]);\n    char* index_file = argv[7];\n\n    rabitqlib::MetricType metric_type = rabitqlib::METRIC_L2;\n    if (argc &gt; 8) {\n        std::string metric_str(argv[8]);\n        if (metric_str == \"ip\" || metric_str == \"IP\") {\n            metric_type = rabitqlib::METRIC_IP;\n        }\n    }\n    if (metric_type == rabitqlib::METRIC_IP) {\n        std::cout &lt;&lt; \"Metric Type: IP\\n\";\n    } else if (metric_type == rabitqlib::METRIC_L2) {\n        std::cout &lt;&lt; \"Metric Type: L2\\n\";\n    }\n\n    bool faster_quant = false;\n    if (argc &gt; 9) {\n        std::string faster_str(argv[9]);\n        if (faster_str == \"true\") {\n            faster_quant = true;\n            std::cout &lt;&lt; \"Using faster quantize for indexing...\\n\";\n        }\n    }\n\n    data_type data;\n    data_type centroids;\n    gt_type cluster_id;\n\n    rabitqlib::load_vecs&lt;float, data_type&gt;(data_file, data);\n    rabitqlib::load_vecs&lt;float, data_type&gt;(centroid_file, centroids);\n    rabitqlib::load_vecs&lt;uint32_t, gt_type&gt;(cid_file, cluster_id);\n\n    size_t num_points = data.rows();\n    size_t dim = data.cols();\n\n    size_t random_seed = 100;  // by default 100\n    auto* hnsw = new rabitqlib::hnsw::HierarchicalNSW(\n        num_points, dim, total_bits, m, ef, random_seed, metric_type\n    );\n\n    rabitqlib::StopW stopw;\n    stopw.reset();\n\n    hnsw-&gt;construct(\n        centroids.rows(),\n        centroids.data(),\n        num_points,\n        data.data(),\n        cluster_id.data(),\n        0,\n        faster_quant\n    );\n\n    float total_time = stopw.get_elapsed_micro();\n    total_time /= 1e6;\n\n    std::cout &lt;&lt; \"indexing time = \" &lt;&lt; total_time &lt;&lt; \"s\" &lt;&lt; '\\n';\n    hnsw-&gt;save(index_file);\n\n    std::cout &lt;&lt; \"index saved...\" &lt;&lt; '\\n';\n\n    return 0;\n}\n</code></pre>"},{"location":"quick_start/#example-code-in-c-for-querying_1","title":"Example Code in C++ for querying","text":"<p>Third, load index, query and groundtruth files to test ANN Search.</p> <pre><code>#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\n#include \"index/hnsw/hnsw.hpp\"\n#include \"utils/io.hpp\"\n#include \"utils/stopw.hpp\"\n\nstd::vector&lt;size_t&gt; efs = {10,  20,  40,  50,  60,  80,  100, 150,  170,  190, 200,\n                           250, 300, 400, 500, 600, 700, 800, 1000, 1500, 2000};\n\nsize_t test_round = 3;\nsize_t topk = 10;\n\nusing PID = rabitqlib::PID;\nusing index_type = rabitqlib::hnsw::HierarchicalNSW;\nusing data_type = rabitqlib::RowMajorArray&lt;float&gt;;\nusing gt_type = rabitqlib::RowMajorArray&lt;uint32_t&gt;;\n\nint main(int argc, char* argv[]) {\n    if (argc &lt; 4) {\n        std::cerr &lt;&lt; \"Usage: \" &lt;&lt; argv[0] &lt;&lt; \" &lt;arg1&gt; &lt;arg2&gt; &lt;arg3&gt; &lt;arg4&gt;\\n\"\n                  &lt;&lt; \"arg1: path for index \\n\"\n                  &lt;&lt; \"arg2: path for query file, format .fvecs\\n\"\n                  &lt;&lt; \"arg3: path for groundtruth file format .ivecs\\n\"\n                  &lt;&lt; \"arg4: metric type (\\\"l2\\\" or \\\"ip\\\")\\n\";\n        exit(1);\n    }\n\n    char* index_file = argv[1];\n    char* query_file = argv[2];\n    char* gt_file = argv[3];\n\n    data_type query;\n    gt_type gt;\n    rabitqlib::load_vecs&lt;float, data_type&gt;(query_file, query);\n    rabitqlib::load_vecs&lt;uint32_t, gt_type&gt;(gt_file, gt);\n    size_t nq = query.rows();\n    size_t total_count = nq * topk;\n\n    index_type hnsw;\n    rabitqlib::MetricType metric_type = rabitqlib::METRIC_L2;\n    if (argc &gt; 4) {\n        std::string metric_str(argv[4]);\n        if (metric_str == \"ip\" || metric_str == \"IP\") {\n            metric_type = rabitqlib::METRIC_IP;\n        }\n    }\n    if (metric_type == rabitqlib::METRIC_IP) {\n        std::cout &lt;&lt; \"Metric Type: IP\\n\";\n    } else if (metric_type == rabitqlib::METRIC_L2) {\n        std::cout &lt;&lt; \"Metric Type: L2\\n\";\n    }\n\n    hnsw.load(index_file, metric_type);\n\n    rabitqlib::StopW stopw;\n\n    auto nefs = efs;\n\n    size_t length = nefs.size();\n\n    std::vector&lt;std::vector&lt;float&gt;&gt; all_qps(test_round, std::vector&lt;float&gt;(length));\n    std::vector&lt;std::vector&lt;float&gt;&gt; all_recall(test_round, std::vector&lt;float&gt;(length));\n\n    std::cout &lt;&lt; \"search start &gt;.....\\n\";\n\n    for (size_t i_probe = 0; i_probe &lt; length; ++i_probe) {\n        for (size_t r = 0; r &lt; test_round; r++) {\n            size_t ef = nefs[i_probe];\n            size_t total_correct = 0;\n            float total_time = 0;\n\n            auto start = std::chrono::high_resolution_clock::now();\n\n            std::vector&lt;std::vector&lt;std::pair&lt;float, PID&gt;&gt;&gt; res =\n                hnsw.search(query.data(), nq, topk, ef, 1);\n\n            auto end = std::chrono::high_resolution_clock::now();\n\n            float elapsed_us =\n                std::chrono::duration&lt;float, std::micro&gt;(end - start).count();\n\n            total_time += elapsed_us;\n\n            for (size_t i = 0; i &lt; nq; i++) {\n                for (size_t j = 0; j &lt; topk; j++) {\n                    for (size_t k = 0; k &lt; topk; k++) {\n                        if (gt(i, k) == res[i][j].second) {\n                            total_correct++;\n                            break;\n                        }\n                    }\n                }\n            }\n\n            float qps = static_cast&lt;float&gt;(nq) / ((total_time) / 1e6F);\n\n            float recall =\n                static_cast&lt;float&gt;(total_correct) / static_cast&lt;float&gt;(total_count);\n\n            all_qps[r][i_probe] = qps;\n            all_recall[r][i_probe] = recall;\n        }\n    }\n\n    auto avg_qps = rabitqlib::horizontal_avg(all_qps);\n    auto avg_recall = rabitqlib::horizontal_avg(all_recall);\n\n    std::cout &lt;&lt; \"EF\\tQPS\\tRecall\\t\"\n\n                 \"\\n\";\n    for (size_t i = 0; i &lt; avg_qps.size(); ++i) {\n        std::cout &lt;&lt; efs[i] &lt;&lt; '\\t' &lt;&lt; avg_qps[i] &lt;&lt; '\\t' &lt;&lt; avg_recall[i] &lt;&lt; '\\t' &lt;&lt; '\\n';\n    }\n}\n</code></pre>"},{"location":"quick_start/#rabitq-qg-symphonyqg","title":"RaBitQ + QG (SymphonyQG)","text":"<p>QG is a graph-based index originated from the NGT library. Different from HNSW, it creates multiple quantization codes for every vector and carefully re-organizes their layout to minimize random memory accesses in querying. RaBitQ + QG in developped from our research project SymphonyQG. Unlike IVF + RaBitQ and HNSW + RaBitQ, which consumes less memory than the raw datasets, RaBitQ + QG consumes more memory to pursue the best time-accuracy trade-off.</p>"},{"location":"quick_start/#example-code-in-c_1","title":"Example Code in C++","text":"<p>Combine graph-based index with RaBitQ and FastScan. <pre><code>#include &lt;random&gt;\n#include &lt;vector&gt;\n\n#include \"index/symqg/qg.hpp\"\n#include \"index/symqg/qg_builder.hpp\"\n#include \"utils/stopw.hpp\"\n\nusing PID = rabitqlib::PID;\n\nint main() {\n    size_t dim = 128;\n    size_t N = 16000;\n\n    std::vector&lt;float&gt; data(dim * N);\n\n    static std::random_device rd;\n    static std::mt19937 gen(rd());\n    std::normal_distribution&lt;float&gt; dist(0, 1);\n\n    // generate N random data vectors\n    for (size_t i = 0; i &lt; dim * N; i++) {\n        data[i] = dist(gen);\n    }\n\n    size_t degree = 32;  // degree bound for graph\n    size_t ef = 200;     // size of search window for indexing\n\n    rabitqlib::symqg::QuantizedGraph&lt;float&gt; qg(N, dim, degree);  // init index\n\n    rabitqlib::symqg::QGBuilder builder(qg, ef, data.data());  // builder of index\n\n    rabitqlib::StopW stopw;\n    stopw.reset();\n\n    builder.build();  // construct index\n\n    float total_time = stopw.get_elapsed_micro();\n    total_time /= 1e6;\n\n    std::cout &lt;&lt; \"indexing time = \" &lt;&lt; total_time &lt;&lt; \"s\" &lt;&lt; '\\n';\n\n    size_t nq = 10;\n    std::vector&lt;float&gt; query(nq * dim);\n\n    // take first nq data vectors as query vectors\n    for (size_t i = 0; i &lt; nq * dim; i++) {\n        query[i] = data[i];\n    }\n\n    size_t topk = 10;\n    size_t ef_search = 100;\n\n    qg.set_ef(ef_search);\n    std::vector&lt;PID&gt; results(topk);\n\n    for (size_t qid = 0; qid &lt; nq; qid++) {\n        qg.search(query.data() + (dim * qid), topk, results.data());\n        std::cout &lt;&lt; \"query \" &lt;&lt; qid &lt;&lt; \"'s \" &lt;&lt; topk &lt;&lt; \"NNs:\" &lt;&lt; '\\n';\n        for (size_t i = 0; i &lt; topk; i++) {\n            std::cout &lt;&lt; \"{ID: \" &lt;&lt; results[i] &lt;&lt; \"}  \";\n        }\n        std::cout &lt;&lt; '\\n';\n    }\n\n    return 0;\n}\n</code></pre></p>"},{"location":"index/hnsw/","title":"HNSW + RaBitQ","text":"<p>HNSW is a popular graph-based index. HNSW + RaBitQ consumes the more memory than IVF + RaBitQ because it needs to store the edges of every vertex in a graph (e.g., 32 edges = 1,024 bits). In terms of the time-accuracy trade-off, HNSW + RaBitQ and IVF + RaBitQ perform differently across datasets\u2014sometimes the former works better, and sometimes the latter does. This document describes how the library integrates HNSW with RaBitQ to support efficient vector search.</p>"},{"location":"index/hnsw/#index-construction","title":"Index Construction","text":"<p>We build the HNSW graph by incrementally inserting new elements following the standard HNSW routine. Currently, we support building the index using raw data vectors and storing the corresponding quantization codes.</p> <p>Users can invoke:</p> <pre><code>HierarchicalNSW::construct(size_t cluster_num,\n                          const float* centroids,\n                          size_t data_num,\n                          const float* data,\n                          PID* cluster_ids,\n                          size_t num_threads = 0,\n                          bool faster = false);\n</code></pre> <ul> <li>data: Pointer to the raw data vectors.</li> <li>data_num: The number of data vectors.</li> <li>centroids: Centroids computed by K-means clustering on the raw data vectors (we recommend <code>cluster_num = 16</code>).  </li> <li>cluster_ids: Array of length <code>data_num</code> where each entry indicates the centroid ID (0\u201315) for the corresponding data vector.  </li> <li>num_threads: Number of threads to use (default: 0, which auto-selects).</li> <li>faster: If <code>true</code>, enales fast quantizer.</li> </ul> <p>During construction, we first rotate the centroids and then insert each element one by one. For each element:</p> <ol> <li>Update the graph structure (edges) by searching with raw vectors and pruning.  </li> <li>Quantize the rotated vector and store its quantization code.</li> </ol>"},{"location":"index/hnsw/#data-layout","title":"Data Layout","text":"<p>Each indexed element is stored in the following layout:</p> <pre><code>[number of edges]\n[edges]\n[cluster ID]\n[external label]\n[BinData (1-bit * dim + factors)]\n[ExData (ex-bits * dim + factors)]\n</code></pre>"},{"location":"index/hnsw/#querying","title":"Querying","text":"<p>Users can invoke: <pre><code>std::vector&lt;std::vector&lt;std::pair&lt;float, PID&gt;&gt;&gt; HierarchicalNSW::search(const float* queries,\n                                                                        size_t query_num,\n                                                                        size_t TOPK,\n                                                                        size_t efSearch,\n                                                                        size_t thread_num);\n</code></pre></p> <ul> <li>queries: Pointer to the raw query vectors.</li> <li>query_num: The number of query vectors.</li> <li>TOPK: The number of nearest neighbors to search.</li> <li>efSearch: The size of the candidate set for searching HNSW base layer.</li> <li>thread_num: Number of threads to use. Each query is processed by one thread.</li> </ul> <p>We first pre-process the query:</p> <ol> <li>Rotate the raw query vector.  </li> <li>Compute distances between the rotated query and all rotated centroids.  </li> <li>Encapsulate the query into a <code>query_wrapper</code> for subsequent search.  </li> </ol>"},{"location":"index/hnsw/#upper-layers","title":"Upper Layers","text":"<p>In the upper layers of HNSW, we compute the 1-bit estimated distance (using <code>BinData</code>) to quickly locate the entry point for the next layer.</p>"},{"location":"index/hnsw/#base-layer","title":"Base Layer","text":"<p>In the base layer, we apply an adaptive re-ranking strategy:</p> <ul> <li>candidate_set: Elements to be visited.  </li> <li>boundedKNN: Current best TOPK candidates.</li> </ul> <p>Repeat until <code>candidate_set</code> is empty:</p> <ol> <li>Extract the nearest element <code>e</code> from <code>candidate_set</code>.  </li> <li>Visit all unvisited neighbors of <code>e</code>.  </li> <li>For each neighbor:</li> <li>Compute the 1-bit lower-bound distance (along with 1-bit estimated distance) using <code>BinData</code>.  </li> <li>If <code>boundedKNN</code> has fewer than TOPK elements, or if the 1-bit lower-bound is smaller than the full-bits estimated distance of the current farthest element in <code>boundedKNN</code>:<ol> <li>Refine the distance estimate using <code>ExData</code> to obtain the full-bits estimated distance.  </li> <li>Update <code>boundedKNN</code>.  </li> </ol> </li> <li>Insert the neighbor into <code>candidate_set</code> with its (possibly refined) estimated distance.  </li> </ol> <p>The search terminates when <code>candidate_set</code> is empty.</p>"},{"location":"index/ivf/","title":"IVF + RaBitQ","text":"<p>IVF is a classical clustering-based method for ANN. It has tiny space consumption. When it is combined with RaBitQ and FastScan, it produces promising time-accuracy trade-off for vector search. This part describes how the library combines IVF with RaBitQ.</p> <p>The algorithm includes two phases: indexing and querying.</p>"},{"location":"index/ivf/#index-construction","title":"Index Construction","text":"<p>The first step is to run a clustering algorithm to partition raw data vectors (<code>*.fvecs</code> format) into different buckets. The algorithm performs KMeans clustering on raw vectors based on Faiss (see <code>python/ivf.py</code>). To run the algorithm, you need to execute the command in <code>shell</code>:</p> <p><pre><code>python python/ivf.py  /path/to/raw/data \\\n                      number_of_clusters \\\n                      /path/to/output/centroids \\\n                      /path/to/output/cluster_ids \\\n                      distance_metric\n</code></pre> For example, the following command splits the sift vector data into 4096 clusters using Euclidean (l2) distance: <pre><code>python python/ivf.py /data/sift/sift_base.fvecs \\\n                     4096 \\\n                     /data/sift/sift_centroids_4096_l2.fvecs \\\n                     /data/sift/sift_clusterids_4096_l2.ivecs \\\n                     l2\n</code></pre></p> <p>After files are prepared, you need to load them into memory: <pre><code>using data_type = rabitqlib::RowMajorArray&lt;float&gt;;\nusing gt_type = rabitqlib::RowMajorArray&lt;uint32_t&gt;;\n\ndata_type data;\ndata_type centroids;\ngt_type cids;\n\nrabitqlib::load_vecs&lt;float, data_type&gt;(data_file, data);\nrabitqlib::load_vecs&lt;float, data_type&gt;(centroids_file, centroids);\nrabitqlib::load_vecs&lt;PID, gt_type&gt;(cids_file, cids);\n</code></pre></p> <p>Then, you need to initialize an IVF object using (1) the number of data points, (2) the dimension of each vector, (3) the number of clusters , and (4) the total bits used to quantize each vector. For example:</p> <pre><code>using index_type = rabitqlib::ivf::IVF;\n\nsize_t num_points = data.rows();\nsize_t dim = data.cols();\nsize_t k = centroids.rows();\n\nindex_type ivf(num_points, dim, k, total_bits);\n</code></pre> <p>Finally, call the construct API: <pre><code>void IVF::construct(\n    const float* data, \n    const float* centroids, \n    const PID* cluster_ids, \n    bool faster = false\n);\n</code></pre></p> <ul> <li>data: Pointer to the raw data vectors.</li> <li>centroids: Centroids computed by K-means clustering on the raw data vectors (we recommend to tune cluster_num around 4 * the square root of the dataset following Faiss).</li> <li>cluster_ids: Array of length data_num where each entry indicates the centroid ID (0\u201315) for the corresponding data vector.</li> <li>faster: If true, enable fast implementations for RaBitQ (By default, it is set as <code>false</code> to pursue better accuracy.).</li> </ul> <p>For example: <pre><code>ivf.construct(data.data(), centroids.data(), cids.data(), true);\n</code></pre></p> <p>During the construction phase, we quantize each cluster in parallel. For each cluster, we first rotate the centroid and vectors in this cluster using a random matrix, then compute the 1-bit codes and (total_bits - 1)-bit ex codes along with corresponding factors.</p> <p>After construction, you can directly save the index file to disk: <pre><code>ivf.save(outoput_index_file);\n</code></pre></p>"},{"location":"index/ivf/#data-layout","title":"Data Layout","text":"<p>The main data layout for our IVF is organized as follows: <pre><code>[batch data]    // 1-bit code and factors\n[ex_data]       // code for remaining bits\n[ids]           // PID of vectors (organized by clusters)\n[cluster_lst]   // List of clusters' metadata in IVF\n</code></pre></p>"},{"location":"index/ivf/#querying","title":"Querying","text":"<p>Currently, querying requires the index to be loaded in memory. If you want to use a previously saved index on the disk,  firstly load it into memory:</p> <p><pre><code>using index_type = rabitqlib::ivf::IVF;\nindex_type ivf;\nivf.load(index_file);\n</code></pre> Once the index is loaded, you can call the search function for queries: <pre><code>void IVF::search(\n    const float* __restrict__ query, \n    size_t k, \n    size_t nprobe, \n    PID* __restrict__ results,\n    bool use_hacc\n) const;\n</code></pre></p> <ul> <li>query: Query vector.</li> <li>k: Top-k.</li> <li>nprobe: The number of closest clusters to search.</li> <li>results: Result buffer, size of k.</li> <li>use_hacc: If use high accuracy FastScan, true by default. For data quantized by high number of bits (e.g., &gt;3), we recommend to use high accuracy FastScan to reduce the error caused by FastScan. Also, user may disable it to improve the query efficiency.</li> </ul> <p>During the search phase, we first rotate the query vector and compute distances between the query vector and the clusters' centroids. Then, we select the n (nprobe) clusters with the smallest distances for search. For each cluster, we first use FastScan to get the coarse distance. Then, if the accuracy of the coarse distance is insufficient, we access the remaining ex bits to boost the accuracy. The search terminates when all selected clusters are scanned and returns the top k nearest neighbours for the given query.</p>"},{"location":"index/qg/","title":"QG + RaBitQ (SymphonyQG)","text":"<p>QG is a graph-based index originated from the NGT library. Different from HNSW, it creates multiple quantization codes for every vector and carefully re-organizes their layout to minimize random memory accesses in querying. RaBitQ + QG in developped from our research project SymphonyQG. Unlike IVF + RaBitQ and HNSW + RaBitQ, which consumes less memory than the raw datasets, RaBitQ + QG consumes more memory to pursue the best time-accuracy trade-off. Here, we offer a toy example for the indexing and querying of QG. To test QG on real-world datasets, please refer to <code>sample/symqg_indexing.cpp</code> and <code>sample/symqg_querying.cpp</code> for detailed information</p>"},{"location":"index/qg/#index-construction","title":"Index Construction","text":"<p>We build the QG by iteratively refining the graph structure. Since the QG is more complicated than other indices, we need a QGBuilder to help us construct the index.</p> <p>At the beginning, we need to intialize a QG and a QGBuilder by following construtor. <pre><code>QuantizedGraph::QuantizedGraph(\n        size_t num,\n        size_t dim,\n        size_t max_deg,\n        RotatorType type = RotatorType::FhtKacRotator\n    );\n\nQGBuilder::QGBuilder(\n        QuantizedGraph&lt;float&gt;&amp; index,\n        uint32_t ef_build,\n        const float* data,\n        size_t num_threads = std::numeric_limits&lt;size_t&gt;::max()\n    )\n</code></pre> - num: Number of vertices (vectors) in the dataset. - dim: Dimension of the dataset. - max_deg: Degree bound of QG, must be a multiple of 32. - index: Previously initialized QG. - ef_build: Search window size during indexing. - data: Pointer to the dataset, size of num * dim. - num_threads: Number of threads to use (default: std::numeric_limits::max(), which auto-selects). <pre><code>size_t rows = 1000000;\nsize_t cols = 128;\nsize_t degree = 32;\nsize_t ef = 200\n\nfloat* data = new float[rows * cols]; // only for illustration\n\nQuantizedGraph qg(rows, cols, degree); // init qg\n\nQGBuilder builder(qg, ef, data.data()); // init builder\n</code></pre> <p>Then, we can use the builder to construct the index. Then we can save the index. <pre><code>builder.build();    // build index interatively\n\nconst char* index_file = \"./qg_example.index\"\nqg.save(index_file);    // save index\n</code></pre></p>"},{"location":"index/qg/#data-layout","title":"Data Layout","text":"<p>Each indexed element is stored in the following layout. <pre><code>[Raw data vector]\n[Batch data for QG]\n[Edges]\n</code></pre></p>"},{"location":"index/qg/#querying","title":"Querying","text":"<p>For querying, code is pretty simple. <pre><code>void QuantizedGraph::search(\n    const T* __restrict__ query, \n    uint32_t k, \n    uint32_t* __restrict__ results);\n</code></pre> - query: Query vector. - k: Top-k. - results: Result buffer, size of k. Then we can use a pre-constructed index to search. <pre><code>QuantizedGraph&lt;float&gt; qg;\nqg.load(\"./qg_example.index\"); // load pre-constructed index\n\n\nsize_t ef = 100;\nsize_t topk = 10;\nstd::vector&lt;PID&gt; results(topk); // result buffer\nfloat* query = new float[cols]; // query vector (only for illustration)\n\nqg.set_ef(ef);  // set search window size\nqg.search(query, topk, results.data()); // search knn, result will be stored in results\n</code></pre></p>"},{"location":"rabitq/estimator/","title":"Estimator","text":"<p>This part presents how to use the quantized vectors to estimate Euclidean distances and inner product with minimal efforts. </p> Notation Description \\mathbf{o}_r, \\mathbf{q}_r The raw data vector and the raw query vector. \\mathbf{c} The center vector. D The dimension of vectors. B The number of bits per dimension used for the quantization. c_B A constant value c_B=- \\frac{2^B-1}{2}. \\mathbf{1}_D The all-one vector of dimension D. P The random rotation matrix. \\mathbf{\\bar o} The quantized normalized data vector. \\mathbf{q}_r' The reversely rotated raw query vector: \\mathbf{q}_r'=P^{-1}\\mathbf{q}_r. S_q S_q=\\sum_{i=1}^{D} \\mathbf{q}'_{r}[i]. \\Delta_x The rescaling factor of a quantized vector. \\mathbf{x}_u The uint representation of the quantized vectors."},{"location":"rabitq/estimator/#quantization","title":"Quantization","text":"<p>Recall that the quantization algorithm of RaBitQ receives a raw data vector \\mathbf{o}_r, a center vector \\mathbf{c} and a sample of random rotation matrices P as inputs and returns a value \\Delta_x and an uint vector \\mathbf{x}_u as output, such that </p>  \\begin{align} \\mathbf{o}_r - \\mathbf{c} \\approx \\| \\mathbf{o}_r-\\mathbf{c} \\| \\cdot \\mathbf{\\bar o} = \\Delta_x P(\\mathbf{x}_u+c_B\\mathbf{1}_D)  \\end{align}  <p>where c_B=- \\frac{2^B-1}{2} and B is the number of bits used for the quantization. \\mathbf{1}_D is the all-one vector of dimension D. P is a sample of random rotation matrices. </p>"},{"location":"rabitq/estimator/#estimator_1","title":"Estimator","text":"<p>The following derivation covers the estimators for Euclidean distances, inner products, and cosine similarity. The cosine similarity is supported by the same estimator as the inner product. </p>"},{"location":"rabitq/estimator/#estimator-of-euclidean-distance","title":"Estimator of Euclidean Distance","text":"\\begin{align} &amp;\\| \\mathbf{o}_r-\\mathbf{q}_r\\|^2  \\\\=&amp; \\|\\mathbf{o}_r -\\mathbf{c}\\|^2 + \\|\\mathbf{q}_r -\\mathbf{c}\\|^2 - 2 \\left&lt; \\mathbf{o}_r - \\mathbf{c}, \\mathbf{q}_r - \\mathbf{c}\\right&gt; \\\\ =&amp; \\|\\mathbf{o}_r -\\mathbf{c}\\|^2 + \\|\\mathbf{q}_r -\\mathbf{c}\\|^2 - 2\\| \\mathbf{o}_r-\\mathbf{c}\\|\\cdot \\| \\mathbf{q}_r-\\mathbf{c}\\| \\cdot \\left&lt;\\mathbf{ o},\\mathbf{q} \\right&gt; \\\\ \\approx&amp; \\|\\mathbf{o}_r -\\mathbf{c}\\|^2 + \\|\\mathbf{q}_r -\\mathbf{c}\\|^2 - 2\\| \\mathbf{o}_r-\\mathbf{c}\\|\\cdot \\| \\mathbf{q}_r-\\mathbf{c}\\| \\cdot \\frac{\\left&lt;\\mathbf{\\bar o},\\mathbf{q} \\right&gt;}{ \\left&lt;\\mathbf{\\bar o},\\mathbf{o} \\right&gt;} \\\\ =&amp; \\|\\mathbf{o}_r -\\mathbf{c}\\|^2 + \\|\\mathbf{q}_r -\\mathbf{c}\\|^2 - 2\\| \\mathbf{o}_r-\\mathbf{c}\\| \\cdot \\frac{\\left&lt;\\mathbf{\\bar o},\\mathbf{q}_r-\\mathbf{c} \\right&gt;}{\\left&lt;\\mathbf{\\bar o},\\mathbf{o} \\right&gt;} \\\\ =&amp; \\|\\mathbf{o}_r -\\mathbf{c}\\|^2 + \\|\\mathbf{q}_r -\\mathbf{c}\\|^2 +2\\| \\mathbf{o}_r-\\mathbf{c}\\| \\frac{\\left&lt; \\mathbf{\\bar o}, \\mathbf{c}\\right&gt;}{\\left&lt;\\mathbf{\\bar o},\\mathbf{o} \\right&gt;} - \\frac{2}{\\left&lt;\\mathbf{\\bar o},\\mathbf{o} \\right&gt;}\\cdot \\left&lt; \\| \\mathbf{o}_r-\\mathbf{c}\\|\\cdot P^{-1}\\mathbf{\\bar o}, \\mathbf{q}_r' \\right&gt; \\\\=&amp;\\|\\mathbf{o}_r -\\mathbf{c}\\|^2 + \\|\\mathbf{q}_r -\\mathbf{c}\\|^2 +2\\| \\mathbf{o}_r-\\mathbf{c}\\| \\frac{\\left&lt; \\mathbf{\\bar o}, \\mathbf{c}\\right&gt;}{\\left&lt;\\mathbf{\\bar o},\\mathbf{o} \\right&gt;} - \\frac{2\\Delta_x}{\\left&lt;\\mathbf{\\bar o},\\mathbf{o} \\right&gt;}\\cdot \\left[ \\left&lt; \\mathbf{x}_u, \\mathbf{q}_r' \\right&gt;+c_B S_q\\right] \\\\ \\| \\mathbf{o}_r-\\mathbf{q}_r\\|^2 \\approx&amp;\\|\\mathbf{o}_r -\\mathbf{c}\\|^2 + \\|\\mathbf{q}_r -\\mathbf{c}\\|^2 +2\\| \\mathbf{o}_r-\\mathbf{c}\\| \\frac{\\left&lt; \\mathbf{\\bar o}, \\mathbf{c}\\right&gt;}{\\left&lt;\\mathbf{\\bar o},\\mathbf{o} \\right&gt;} - \\frac{2\\Delta_x}{\\left&lt;\\mathbf{\\bar o},\\mathbf{o} \\right&gt;}\\cdot \\left[ \\left&lt; \\mathbf{x}_u, \\mathbf{q}_r' \\right&gt;+c_B S_q\\right] \\end{align}  <p>The error bound is given by </p>  \\begin{align} % \\pm \\ 2 \\|\\mathbf{o}_r-\\mathbf{c}\\|\\cdot \\| \\mathbf{q}_r-\\mathbf{c}\\| \\cdot \\frac{c_{error}}{\\sqrt{D}\\cdot 2^{B-1}}\\\\ \\pm \\ 2 \\|\\mathbf{o}_r-\\mathbf{c}\\|\\cdot \\| \\mathbf{q}_r-\\mathbf{c}\\| \\cdot \\sqrt{\\frac{1 - \\left&lt; \\mathbf{\\bar o},\\mathbf{o}\\right&gt;^2}{\\left&lt; \\mathbf{\\bar o},\\mathbf{o}\\right&gt;^2}} \\frac{\\epsilon_{0}}{\\sqrt{D-1}} \\end{align}  <p>Here \\epsilon_0 is a tunable parameter which controls the confidence level of the error bound. By default, \\epsilon_0=1.9 guarantees nearly perfect confidence. </p> <p>We store the following variables such that the estimator can be computed easily.</p> Name (Type) of Variable Description <code>F_add (float)</code> \\|\\mathbf{o}_r-\\mathbf{c}\\|^2+2\\| \\mathbf{o}_r-\\mathbf{c}\\| \\frac{\\left&lt; \\mathbf{\\bar o}, \\mathbf{c}\\right&gt;}{\\left&lt;\\mathbf{\\bar o},\\mathbf{o} \\right&gt;} <code>F_rescale (float)</code> -2\\frac{\\Delta_x}{\\left&lt;\\mathbf{\\bar o},\\mathbf{o} \\right&gt;} <code>F_error (float)</code> \\ 2 \\|\\mathbf{o}_r-\\mathbf{c}\\| \\cdot \\sqrt{\\frac{1 - \\left&lt; \\mathbf{\\bar o},\\mathbf{o}\\right&gt;^2}{\\left&lt; \\mathbf{\\bar o},\\mathbf{o}\\right&gt;^2}} \\frac{\\epsilon_{0}}{\\sqrt{D-1}} <code>G_add (float)</code> \\|\\mathbf{q}_r-\\mathbf{c}\\|^2 <code>G_kBxSumq (float)</code> c_BS_q <code>G_error (float)</code> \\| \\mathbf{q}_r-\\mathbf{c}\\|"},{"location":"rabitq/estimator/#estimator-of-inner-product","title":"Estimator of Inner Product","text":"<p>When inner product is used as the metric of vector search, it targets the data vector which has the maximum inner product with the query vector. To unify the question with nearest neighbor search, we follows Faiss and hnswlib to compute the negative inner product. </p>  \\begin{align} &amp;-\\left&lt; \\mathbf{o}_r,\\mathbf{q}_r\\right&gt; \\\\=&amp; -\\left&lt; \\mathbf{o}_r-\\mathbf{c} + \\mathbf{c},\\mathbf{q}_r-\\mathbf{c} + \\mathbf{c}\\right&gt; \\\\=&amp; -\\left&lt; \\mathbf{q}_r,\\mathbf{c}\\right&gt; -\\left&lt; \\mathbf{o}_r-\\mathbf{c},\\mathbf{c}\\right&gt; -  \\left&lt; \\mathbf{o}_r-\\mathbf{c},\\mathbf{q}_r-\\mathbf{c} \\right&gt; \\\\ \\approx &amp;-\\left&lt; \\mathbf{q}_r,\\mathbf{c}\\right&gt; -\\left&lt; \\mathbf{o}_r-\\mathbf{c},\\mathbf{c}\\right&gt; +\\| \\mathbf{o}_r-\\mathbf{c}\\| \\frac{\\left&lt; \\mathbf{\\bar o}, \\mathbf{c}\\right&gt;}{\\left&lt;\\mathbf{\\bar o},\\mathbf{o} \\right&gt;} - \\frac{\\Delta_x}{\\left&lt;\\mathbf{\\bar o},\\mathbf{o} \\right&gt;}\\cdot \\left[ \\left&lt; \\mathbf{x}_u, \\mathbf{q}_r' \\right&gt;+c_B S_q\\right] \\\\ -\\left&lt; \\mathbf{o}_r,\\mathbf{q}_r\\right&gt;\\approx &amp;-\\left&lt; \\mathbf{q}_r,\\mathbf{c}\\right&gt; -\\left&lt; \\mathbf{o}_r-\\mathbf{c},\\mathbf{c}\\right&gt; +\\| \\mathbf{o}_r-\\mathbf{c}\\| \\frac{\\left&lt; \\mathbf{\\bar o}, \\mathbf{c}\\right&gt;}{\\left&lt;\\mathbf{\\bar o},\\mathbf{o} \\right&gt;} - \\frac{\\Delta_x}{\\left&lt;\\mathbf{\\bar o},\\mathbf{o} \\right&gt;}\\cdot \\left[ \\left&lt; \\mathbf{x}_u, \\mathbf{q}_r' \\right&gt;+c_B S_q\\right] \\end{align}  <p>The error bound is given by </p>  \\begin{align} % \\pm  \\|\\mathbf{o}_r-\\mathbf{c}\\|\\cdot \\| \\mathbf{q}_r-\\mathbf{c}\\| \\cdot \\frac{c_{error}}{\\sqrt{D}\\cdot 2^{B-1}} \\pm \\|\\mathbf{o}_r-\\mathbf{c}\\|\\cdot \\| \\mathbf{q}_r-\\mathbf{c}\\| \\cdot \\sqrt{\\frac{1 - \\left&lt; \\mathbf{\\bar o},\\mathbf{o}\\right&gt;^2}{\\left&lt; \\mathbf{\\bar o},\\mathbf{o}\\right&gt;^2}} \\frac{\\epsilon_{0}}{\\sqrt{D-1}} \\end{align}  <p>Here \\epsilon_0 is a tunable parameter which controls the confidence level of the error bound. By default, \\epsilon_0=1.9 guarantees nearly perfect confidence. </p> <p>We store the following variables such that the estimator can be computed easily. The variables <code>F_</code> indicates the factors for the data vector. The variables <code>G_</code> indicates the factors for the query vector.</p> Name (Type) of Variable Description <code>F_add (float)</code> -\\left&lt; \\mathbf{o}_r-\\mathbf{c},\\mathbf{c}\\right&gt;+\\| \\mathbf{o}_r-\\mathbf{c}\\| \\frac{\\left&lt; \\mathbf{\\bar o}, \\mathbf{c}\\right&gt;}{\\left&lt;\\mathbf{\\bar o},\\mathbf{o} \\right&gt;} <code>F_rescale (float)</code> -\\frac{\\Delta_x}{\\left&lt;\\mathbf{\\bar o},\\mathbf{o} \\right&gt;} <code>F_error (float)</code> \\|\\mathbf{o}_r-\\mathbf{c}\\| \\cdot \\sqrt{\\frac{1 - \\left&lt; \\mathbf{\\bar o},\\mathbf{o}\\right&gt;^2}{\\left&lt; \\mathbf{\\bar o},\\mathbf{o}\\right&gt;^2}} \\frac{\\epsilon_{0}}{\\sqrt{D-1}} <code>G_add (float)</code> -\\left&lt; \\mathbf{q}_r,\\mathbf{c}\\right&gt; <code>G_kBxSumq (float)</code> c_BS_q <code>G_k1xSumq (float)</code> c_1S_q <code>G_error (float)</code> \\| \\mathbf{q}_r-\\mathbf{c}\\|"},{"location":"rabitq/estimator/#implementation","title":"Implementation","text":""},{"location":"rabitq/estimator/#distance-estimation","title":"Distance Estimation","text":"<p>Based on the above derivation, we can implement the estimator of Euclidean distance and inner product using exactly the same implementation. A pseudo code is given below. Let <code>ip</code> be the inner product between the binary code and the randomly rotated query vector.</p> <pre><code>// Compute the estimated distance\n// Note that G_add is dependent on the center vector.\nfloat est_dist = F_add + G_add + F_rescale * (ip + G_kBxSumq)\n\n// Compute the error bound \n// Note that G_error is dependent on the center vector.\nfloat error_bound = F_error * G_error\n\n// Compute the lower and upper bounds of the estimated distance\nfloat lb_dist = est_dist - error_bound\nfloat ub_dist = est_dist + error_bound\n</code></pre>"},{"location":"rabitq/estimator/#incremental-distance-estimation","title":"Incremental Distance Estimation","text":"<p>RaBitQ supports incremental distance estimation. We split the code into two parts: the binary code (the most significant bits) and the extended code (the remaining B-1 bits).  Incremental distance estimation supports to first estimate a coarse distance based on the binary code. If the accuracy is insufficient, we then access the extended code to boost the accuracy.  For this, we need to prepare factors for both the binary code and the extended code. The factors for the binary code are stored in <code>F_add</code>, <code>F_rescale</code> and <code>F_error</code>. The factors for the extended code are stored in <code>F_add_ex</code>, <code>F_rescale_ex</code> and <code>F_error_ex</code>. The factors for the query includes <code>G_add</code>, <code>G_error</code>, <code>G_kBxSumq</code> and <code>G_k1xSumq</code>.  Let <code>ip_bin</code> be the inner product between the binary code and the randomly rotated query vector and <code>ip_ex</code> be the inner product between the ex-code and the randomly rotated query vector. </p> <pre><code>// 1-bit dist\nfloat est_dist = F_add + G_add + F_rescale * (ip_bin + G_k1xSumq)\nfloat bound = F_error * G_error\nfloat ub_dist = est_dist + bound\nfloat lb_dist = est_dist - bound\n\n// boost to full-bit dist\nfloat ex_est_dist = F_add_ex + G_add + F_rescale_ex * (ip_bin &lt;&lt; (bits - 1) + ip_ex + G_kBxSumq)\nfloat ex_bound = F_error_ex * G_error\nfloat ex_ub_dist = ex_est_dist + ex_bound;\nfloat ex_lb_dist = ex_est_dist - ex_bound;\n</code></pre>"},{"location":"rabitq/quantizer/","title":"Quantizer","text":"<p>RaBitQLib includes two versions of implementations for the RaBitQ algorithm and designs various data formats to ease practical deployment. Specifically, the two implementaions offer different trade-offs as follows. </p> <ol> <li>Optimal accuracy with longer quantization time.</li> <li>Nearly-optimal accuracy with significantly fast quantization.</li> </ol> <p>Various advanced data formats are provided to support the following needs. </p> <ol> <li>Drop-in replacement for uniform scalar quantization.</li> <li>Efficient distance estimation for single vectors.</li> <li>Efficient distance estimation for batched vectors.</li> <li>Incremental distance estimation for splitted single vectors.</li> <li>Incremental distance estimation for splitted batched vectors.</li> </ol> <p>Note that these data formats only map raw floating-point vectors into codes of <code>uint8</code>/<code>uint32</code> arrays. To compactly store the code vector, please further refer to <code>rabitqlib/quantization/pack_ex_code.hpp</code>.</p> <p>RaBitQ quantizer is included in <code>rabitq_impl.hpp</code> and <code>rabitq.hpp</code>. <pre><code>.\n\u251c\u2500\u2500 rabitqlib\n\u2502   \u251c\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 quantization\n\u2502       \u251c\u2500\u2500 ...\n\u2502       \u251c\u2500\u2500 rabitq_impl.hpp\n\u2502       \u2514\u2500\u2500 rabitq.hpp\n\u2514\u2500\u2500 ...\n</code></pre></p>"},{"location":"rabitq/quantizer/#implementations","title":"Implementations","text":"<p>Let B be the bit-width for each dimension. Both implementations of RaBitQ include two steps. </p> <ol> <li>Compute a binary code by recording the sign of every coordinate. </li> <li>Compute an ex-code of B-1 bits (when B&gt;1).</li> </ol> <p>The binary code is easily computed by the function <code>one_bit_code</code> in <code>rabitq_impl.hpp</code>.</p> <p>The computation of ex-codes includes two versions of implementation. </p> <p>In the first implementation, we compute the ex-codes of RaBitQ based on the algorithm described in the RaBitQ paper (Section 3.2.2). For a vector, to minimize the quantization error, the algorithm tries many different rescaling factors. For each rescaling factor, it rescales the vector and performs rounding (i.e., scalar quantization) to generate a quantization code. Then it finds out the factor and codes which minimizes the quantization error. Note that in the library, the range of enumeration is approriately shrinked, which brings better efficiency without affecting the accuracy. </p> <p>In the second implementation, instead of enumerating different rescaling factors, it directly rounds every vector based on the expected optimal factor. Specifically, recall that all data vectors are randomly rotated before quantization. The expected optimal factor is computed as follows. We sample several random vectors which follow uniform distribution on the unit sphere and use the first implementation to quantize them. We record the optimal factor for each and take the average of the optimal factors as the expected optimal factor. This implementation introduces some accuracy decrease while significantly speeds up the quantization.</p>"},{"location":"rabitq/quantizer/#data-format","title":"Data Format","text":""},{"location":"rabitq/quantizer/#format-1-drop-in-replacement-of-uniform-scalar-quantization","title":"Format 1 - Drop-in Replacement of Uniform Scalar Quantization","text":"<p>This format allows RaBitQ to be used as a direct replacement for uniform scalar quantization, which offers higher accuracy under the same bit-width. The improvement of accuracy is significant when the bit-width is small (B &lt; 6). <pre><code>#include &lt;cstdint&gt;\n#include &lt;random&gt;\n#include &lt;vector&gt;\n\n#include \"quantization/rabitq.hpp\"\n\nint main() {\n    size_t dim = 768;\n\n    std::vector&lt;float&gt; vector(dim);\n    static std::random_device rd;\n    static std::mt19937 gen(rd());\n    std::normal_distribution&lt;float&gt; dist(0, 1);\n\n    // generate a random vector\n    for (size_t i = 0; i &lt; dim; ++i) {\n        vector[i] = dist(gen);\n    }\n\n    size_t bits = 8;                  // num of bits for total code\n    std::vector&lt;uint32_t&gt; code(dim);  // code\n    float delta;                      // delta for scalar quantization\n    float vl;                         // lower value for scalar quantization\n\n    // scalar quantization\n    rabitqlib::quant::quantize_scalar(vector.data(), dim, bits, code.data(), delta, vl);\n\n    // faster version, must init a config struct first\n    rabitqlib::quant::RabitqConfig config = rabitqlib::quant::faster_config(dim, bits);\n    rabitqlib::quant::quantize_scalar(\n        vector.data(), dim, bits, code.data(), delta, vl, config\n    );\n\n    // Note that with this interface, the codes are not compactly stored.\n    // To compactly store the codes, please refer to\n    // `rabitqlib/quantization/pack_excode.hpp`\n\n    return 0;\n}\n</code></pre></p>"},{"location":"rabitq/quantizer/#format-2-distance-estimation-for-a-single-vector","title":"Format 2 - Distance Estimation for a Single Vector","text":""},{"location":"rabitq/quantizer/#indexing","title":"Indexing","text":"<p>This format is designed for computing distance metrics between data vectors and query vectors efficiently. It includes precomputed factors to ease similarity calculations. <pre><code>#include &lt;cstdint&gt;\n#include &lt;random&gt;\n#include &lt;vector&gt;\n\n#include \"quantization/rabitq.hpp\"\n\nint main() {\n    size_t dim = 768;\n\n    std::vector&lt;float&gt; vector(dim);\n    std::vector&lt;float&gt; centroid(dim);\n    static std::random_device rd;\n    static std::mt19937 gen(rd());\n    std::normal_distribution&lt;float&gt; dist(0, 1);\n\n    // generate a random vector\n    for (size_t i = 0; i &lt; dim; ++i) {\n        vector[i] = dist(gen);\n        centroid[i] = dist(gen);\n    }\n\n    size_t bits = 8;                  // num of bits for total code\n    std::vector&lt;uint32_t&gt; code(dim);  // code\n    float f_add;                      // factors for estimating similarity\n    float f_rescale;                  // factors for estimating similarity\n    float f_error;                    // factors for computing error bounds\n\n    rabitqlib::quant::quantize_full_single(\n        vector.data(),\n        centroid.data(),\n        dim,\n        bits,\n        code.data(),\n        f_add,\n        f_rescale,\n        f_error,\n        rabitqlib::METRIC_L2\n    );\n\n    // faster version, must init a config struct first\n    rabitqlib::quant::RabitqConfig config = rabitqlib::quant::faster_config(dim, bits);\n    rabitqlib::quant::quantize_full_single(\n        vector.data(),\n        centroid.data(),\n        dim,\n        bits,\n        code.data(),\n        f_add,\n        f_rescale,\n        f_error,\n        rabitqlib::METRIC_L2,\n        config\n    );\n\n    // Note that with this interface, the codes are not compactly stored.\n    // To compactly store the codes, please refer to\n    // `rabitqlib/quantization/pack_excode.hpp`\n\n    return 0;\n}\n</code></pre></p>"},{"location":"rabitq/quantizer/#querying","title":"Querying","text":"<p>After quantization, we can use the pre-computed quantization codes and factors to get estimated distance for a given query.Here, we separately stored factors and quantization codes, and user may choose to compact them all together to get a improve space locality (e.g., implementation in our index). Also, we did not rotate the vectors in this example for simplicity of illustration. <pre><code>...\n    ...... // we omit the quantization code here\n    std::vector&lt;float&gt; query(dim);\n    for (size_t i = 0; i &lt; dim; ++i) {\n        query[i] = dist(gen);\n    }\n\n    // please refer to estimator.md for defination of these factors\n    float c_1 = -static_cast&lt;float&gt;((1 &lt;&lt; 1) - 1) / 2.F;\n    float k1xsumq = c_1 * std::accumulate(query.begin(), query.end(), 0.F);\n    float g_add = rabitqlib::euclidean_sqr(query.data(), centroid.data(), dim);\n\n    float est_dist = rabitqlib::quant::full_est_dist(\n        code.data(),\n        query.data(),\n        rabitqlib::excode_ipimpl::ip_fxi,\n        dim,\n        bits,\n        f_add,\n        f_rescale,\n        g_add,\n        k1xsumq\n    );\n    float gt_dist = rabitqlib::euclidean_sqr(query.data(), vector.data(), dim);\n\n    std::cout &lt;&lt; \"Estimated distance: \" &lt;&lt; est_dist &lt;&lt; '\\n';\n    std::cout &lt;&lt; \"GT distance: \" &lt;&lt; gt_dist &lt;&lt; '\\n';\n</code></pre></p>"},{"location":"rabitq/quantizer/#format-3-distance-estimation-for-a-batch-of-vectors-for-qg","title":"Format 3 - Distance Estimation for a Batch of Vectors (for QG)","text":"<p>A variant of this data format is used in QG. </p>"},{"location":"rabitq/quantizer/#indexing_1","title":"Indexing","text":"<p>This format is designed for scenarios where distances need to be computed between a query vector and multiple data vectors (quantized into 1-bit per dimension) simultaneously, providing significant performance improvements with FastScan for batch processing.  In practical implementation of our SymphonyQG index, the data is compactly stored. Please refer to <code>QGBatchDataMap</code> in <code>rabitqlib/quantization/data_layout.hpp</code> for detailed information.</p> <pre><code>#include &lt;cstdint&gt;\n#include &lt;random&gt;\n#include &lt;vector&gt;\n\n#include \"quantization/rabitq_impl.hpp\"\n\nint main() {\n    size_t dim = 768;\n    size_t batch_size = 32;  // a batch for FastScan contains 32 vectors\n\n    std::vector&lt;float&gt; vector(dim * batch_size);\n    std::vector&lt;float&gt; centroid(dim);\n    static std::random_device rd;\n    static std::mt19937 gen(rd());\n    std::normal_distribution&lt;float&gt; dist(0, 1);\n\n    // generate a random vector\n    for (size_t i = 0; i &lt; batch_size * dim; ++i) {\n        vector[i] = dist(gen);\n    }\n    for (size_t i = 0; i &lt; dim; ++i) {\n        centroid[i] = dist(gen);\n    }\n\n    // std::vector&lt;uint32_t&gt; code(dim);  // code\n    std::vector&lt;uint8_t&gt; packed_code(batch_size * dim / 8);\n    std::vector&lt;float&gt; f_add(batch_size);      // factors for estimating similarity\n    std::vector&lt;float&gt; f_rescale(batch_size);  // factors for estimating similarity\n    std::vector&lt;float&gt; f_error(batch_size);    // factors for computing error bounds\n\n    rabitqlib::quant::rabitq_impl::one_bit::one_bit_batch_code(\n        vector.data(),\n        centroid.data(),\n        batch_size,\n        dim,\n        packed_code.data(),\n        f_add.data(),\n        f_rescale.data(),\n        f_error.data(),\n        rabitqlib::METRIC_L2\n    );\n\n    return 0;\n}\n</code></pre>"},{"location":"rabitq/quantizer/#querying_1","title":"Querying","text":"<p>During querying, a query is pre-processed as follows. Then FastScan can be called to estimate distance batch by batch. The detailed implementation is in <code>rabitqlib/index/estimator.hpp</code>.  Here, we assume the data are compactly stored in the layout of <code>QGBatchDataMap</code> in <code>rabitqlib/quantization/data_layout.hpp</code>.</p> <pre><code>    size_t dim = 768;  // the dimensionality\n    std::vector&lt;float&gt; rotated_query(dim);\n    std::vector&lt;char&gt; batch_data(rabitqlib::QGBatchDataMap&lt;float&gt;::data_bytes(dim));\n\n    rabitqlib::BatchQuery&lt;float&gt; processed_query(rotated_query.data(), dim);\n\n    // The factors should be set according to the centroid vector.\n    // For ANN, this is preprocessed for every center vector when a query comes.\n    processed_query.set_g_add(\n        std::sqrt(rabitqlib::euclidean_sqr(rotated_query.data(), centroid.data(), dim))\n    );\n\n    size_t batch_size = 32;\n\n    std::vector&lt;float&gt; est_distance(batch_size);  // store the estimated distances\n\n    // We suggest users to customize the kernel if some outputs are not needed.\n    // Eg., QG does not need error bounds, thus we only get the estimated distance here.\n    // If users want to maintain the information of error bound, please refer to our\n    // implementation of IVF index.\n\n    rabitqlib::qg_batch_estdist(\n        batch_data.data(), processed_query, dim, est_distance.data()\n    );\n</code></pre>"},{"location":"rabitq/quantizer/#format-4-incremental-distance-estimation-for-split-single-vectors-for-hnsw","title":"Format 4 - Incremental Distance Estimation for Split Single Vectors (for HNSW)","text":"<p>This format can be used in HNSW. </p>"},{"location":"rabitq/quantizer/#indexing_2","title":"Indexing","text":"<p>This format supports computing distances incrementally when vectors are split across multiple memory locations or when only partial vector information is available at a time.</p> <pre><code>#include &lt;random&gt;\n#include &lt;vector&gt;\n\n#include \"quantization/rabitq.hpp\"\n\nint main() {\n    size_t dim = 768;\n\n    std::vector&lt;float&gt; vector(dim);\n    std::vector&lt;float&gt; centroid(dim);\n    static std::random_device rd;\n    static std::mt19937 gen(rd());\n    std::normal_distribution&lt;float&gt; dist(0, 1);\n\n    // generate a random vector\n    for (size_t i = 0; i &lt; dim; ++i) {\n        vector[i] = dist(gen);\n        centroid[i] = dist(gen);\n    }\n\n    size_t bits = 5;  // num of bits for total code\n    // `bin_data` includes compact binary codes (dim / 8 bytes) and three factors - f_add,\n    // f_rescale and f_error (12 bytes) f_error can be dropped if the error bound is not\n    // used in your index (e.g., QG).\n    // You can also use rabitqlib::BinDataMap&lt;float&gt;::data_bytes(dim) to get the bin data\n    // bytes\n    std::vector&lt;char&gt; bin_data((dim / 8) + 12);\n\n    // `ex_data` includes compact binary codes (dim / 8 bytes) and two factors - f_add_ex,\n    // f_rescale_ex (8 bytes). Here, we drop f_error_ex since it is not used in this index.\n    // You can also use rabitqlib::ExDataMap&lt;float&gt;::data_bytes(dim, bits-1) to get the ex\n    // data bytes\n    std::vector&lt;char&gt; ex_data((dim * (bits - 1) / 8) + 8);\n\n    rabitqlib::quant::quantize_split_single(\n        vector.data(),\n        centroid.data(),\n        dim,\n        bits - 1,\n        bin_data.data(),\n        ex_data.data(),\n        rabitqlib::METRIC_L2\n    );\n\n    // use fast implementation for the data format\n    rabitqlib::quant::RabitqConfig config = rabitqlib::quant::faster_config(dim, bits);\n    rabitqlib::quant::quantize_split_single(\n        vector.data(),\n        centroid.data(),\n        dim,\n        bits - 1,\n        bin_data.data(),\n        ex_data.data(),\n        rabitqlib::METRIC_L2,\n        config\n    );\n\n    return 0;\n}\n</code></pre>"},{"location":"rabitq/quantizer/#querying_2","title":"Querying","text":"<pre><code>...\n    size_t dim = 768;  // the dimensionality\n    size_t bits = 5;   // the bit-width of DATA vectors\n    std::vector&lt;float&gt; rotated_query(dim);\n\n    // the config of fast quantizer is necessary for preprocessing queries\n    rabitqlib::quant::RabitqConfig config = rabitqlib::quant::faster_config(dim, bits);\n\n    rabitqlib::SplitSingleQuery&lt;float&gt; processed_query(\n        rotated_query.data(), dim, bits - 1, config, rabitqlib::METRIC_L2\n    );\n\n    // set factors for distance estimation.\n    // In ANN the factors are precomputed when a query comes.\n    float norm =\n        rabitqlib::euclidean_sqr(rotated_query.data(), centroid.data(), dim);\n    float error = rabitqlib::dot_product(rotated_query.data(), centroid.data(), dim);\n\n    // Compute estimated distances based on binary codes\n    float ip_x0_qr;\n    float est_dist;\n    float low_dist;\n\n    split_single_estdist(\n        bin_data.data(), processed_query, dim, ip_x0_qr, est_dist, low_dist, -norm, error\n    );\n\n    // the kernel of computing inner product between compact codes and query vectors\n    auto ip_func = rabitqlib::select_excode_ipfunc(bits - 1);\n\n    // Compute more accurate distance based on full codes.\n    float est_dist_ex;\n    float low_dist_ex;\n    float ip_x0_qr_ex;\n\n    split_single_fulldist(\n        bin_data.data(),\n        ex_data.data(),\n        ip_func,\n        processed_query,\n        dim,\n        bits - 1,\n        est_dist_ex,\n        low_dist_ex,\n        ip_x0_qr_ex,\n        -norm,\n        error\n    );\n</code></pre>"},{"location":"rabitq/quantizer/#format-5-incremental-distance-estimation-for-split-batched-vectors-for-ivf","title":"Format 5 - Incremental Distance Estimation for Split Batched Vectors (for IVF)","text":"<p>This data format is used in IVF.</p>"},{"location":"rabitq/quantizer/#indexing_3","title":"Indexing","text":"<p>This format combines the benefits of batched processing with incremental computation, allowing efficient distance estimation when dealing with large collections of split vectors.</p> <pre><code>#include &lt;random&gt;\n#include &lt;vector&gt;\n\n#include \"quantization/rabitq.hpp\"\n\nint main() {\n    size_t dim = 768;\n    size_t batch_size = 32;\n\n    std::vector&lt;float&gt; vector(dim * batch_size);\n    std::vector&lt;float&gt; centroid(dim);\n    static std::random_device rd;\n    static std::mt19937 gen(rd());\n    std::normal_distribution&lt;float&gt; dist(0, 1);\n\n    // generate a random vector\n    for (size_t i = 0; i &lt; dim; ++i) {\n        centroid[i] = dist(gen);\n    }\n    for (size_t i = 0; i &lt; dim * batch_size; ++i) {\n        vector[i] = dist(gen);\n    }\n\n    size_t bits = 5;  // num of bits for full codes\n    // `batch_data` includes packed binary codes (dim / 8 bytes) and three factors - f_add,\n    // f_rescale and f_error (12 bytes)\n    // f_error can be dropped if the error bound is not used in your index (e.g., QG)\n    std::vector&lt;char&gt; batch_data((dim / 8 + 12) * batch_size);\n\n    // `ex_data` includes compact binary codes (dim / 8 bytes) and two factors - f_add_ex,\n    // f_rescale_ex (8 bytes). Here, we drop f_error_ex since it is not used in this index.\n    // You can also use rabitqlib::ExDataMap&lt;float&gt;::data_bytes(dim, bits-1) to get the ex\n    // data bytes\n    std::vector&lt;char&gt; ex_data((dim * (bits - 1) / 8 + 8) * batch_size);\n\n    rabitqlib::quant::quantize_split_batch(\n        vector.data(),\n        centroid.data(),\n        batch_size,\n        dim,\n        bits - 1,\n        batch_data.data(),\n        ex_data.data(),\n        rabitqlib::METRIC_L2\n    );\n\n    // use fast implementation for the data format\n    rabitqlib::quant::RabitqConfig config = rabitqlib::quant::faster_config(dim, bits);\n    rabitqlib::quant::quantize_split_batch(\n        vector.data(),\n        centroid.data(),\n        batch_size,\n        dim,\n        bits - 1,\n        batch_data.data(),\n        ex_data.data(),\n        rabitqlib::METRIC_L2,\n        config\n    );\n\n    return 0;\n}\n</code></pre>"},{"location":"rabitq/quantizer/#querying_3","title":"Querying","text":"<pre><code>    size_t dim = 768;  // the dimensionality\n    size_t bits = 5;   // the bit-width of DATA vectors\n    std::vector&lt;float&gt; rotated_query(dim);\n\n    // The flag use_hacc controls the precision of FastScan.\n    // `use_hacc = false` - each number in LUTs is quantized into 8 bits.\n    // `use_hacc = true` - each number in LUTs is quantized into 16 bits.\n    // By default, `use_hacc = true` as it works for all settings of `bits`,\n    // i.e., the bit-width of queries is significantly larger than that for data.\n    // When the bit-width of data &lt;= 2, `use_hacc = false` does not harm accuracy.\n\n    rabitqlib::SplitBatchQuery&lt;float&gt; processed_query(\n        rotated_query.data(), dim, bits - 1, rabitqlib::METRIC_L2, true\n    );\n\n    // The factors should be set according to the centroid vector.\n    // For ANN, this is preprocessed for every center vector when a query comes.\n    processed_query.set_g_add(\n        std::sqrt(rabitqlib::euclidean_sqr(rotated_query.data(), centroid.data(), dim)),\n        rabitqlib::dot_product(rotated_query.data(), centroid.data(), dim)\n    );\n\n    size_t batch_size = 32;\n\n    std::vector&lt;float&gt; est_distance(batch_size);  // store the estimated distances\n    std::vector&lt;float&gt; low_distance(batch_size);  // store the lower bound\n    std::vector&lt;float&gt; ip_x0_qr(batch_size\n    );  // store the intermediate result of inner product\n\n    // We suggest users to customize the kernel if some outputs are not needed.\n    // For example, QG does not need error bounds, see `qg_batch_estdist` in\n    // `rabitqlib/index/estimator.hpp` for details.\n\n    rabitqlib::split_batch_estdist(\n        batch_data.data(),\n        processed_query,\n        dim,\n        est_distance.data(),\n        low_distance.data(),\n        ip_x0_qr.data(),\n        true\n    );\n\n    // the kernel of computing inner product between compact codes and query vectors\n    auto ip_func = select_excode_ipfunc(bits - 1);\n\n    size_t i = 15;\n    split_distance_boosting(\n        ex_data.data() + (i * rabitqlib::ExDataMap&lt;float&gt;::data_bytes(dim, bits - 1)),\n        ip_func,\n        processed_query,\n        dim,\n        bits - 1,\n        ip_x0_qr[i]\n    );\n</code></pre>"},{"location":"rabitq/rabitq/","title":"RaBitQ","text":"<p>The RaBitQ algorithm is a drop-in replacement of binary quantization and (uniform) scalar quantization, with its 1-bit version (released in May 2024) and multi-bit version (released in Sep 2024), respectively.</p> <p>The key advantages of RaBitQ include</p> <ul> <li>High Accuracy with Tiny Space - RaBitQ achieves the state-of-the-art accuracy under diverse space budgets for the estimation of similarity metrics. It produces promising accuracy with even 1-bit per dimension.</li> <li>Fast Distance Estimation - RaBitQ supports to estimate the similarity metrics with high efficiency based on bitwise operations or FastScan.</li> <li>Theoretical Error Bound - RaBitQ provides an asymptotically optimal error bound for the estimation of distances and inner product. The error bound can be used for reliable ordering and reranking.</li> </ul>"},{"location":"rabitq/rabitq/#workflow","title":"Workflow","text":"<p>The RaBitQ algorithm includes two steps:</p> <ol> <li> <p>Random Rotation - Sample a random rotation and apply it to all vectors (including the raw data vectors, the center vector and the raw query vectors). See Rotator for more details.</p> </li> <li> <p>Quantization - After the random rotation, the quantization algorithm quantizes a vector of floating-point numbers into a vector of low-bit unsigned integers. See Quantizer for more details.</p> </li> </ol> <p>After the quantization, we can estimate the similarity metrics including Euclidean distance, inner product and cosine similarity based on the code vector \\mathbf{x}_u and the rescaling factor \\Delta_x. See Estimator for more details.</p>"},{"location":"rabitq/reranking/","title":"Reranking","text":"<p>Reranking is a technique widely adopted for improving recall of vector search. During searching, ANN algorithms usually </p> <ol> <li>shortlist a set of candidates based on an index and their quantization codes (e.g., in memory);</li> <li>retrieve the raw vectors (e.g., from disks) for those candidates which have the smallest estimated distances;</li> <li>computes the exact distances for the candidates to find the nearest neighbors.</li> </ol> <p>RaBitQ has a unique advantage in reranking due to its theoretical error bound. It can skip reranking a candidate if the lower bound of its estimated distance is larger than the upper bound of the distance of the nearest neighbors. </p> <p>Based on error bounds, it is possible to rerank fewer than K vectors to achieve nearly perfect recall - it only reranks the vectors on the boundaries of KNNs.</p>"},{"location":"rabitq/reranking/#algorithm-description","title":"Algorithm Description","text":"<p>Let K be the number of nearest neighbors we target. After receiving the candidates and their estimated distances from an index, e.g., HNSW + RaBitQ, we perform the following strategy of reranking to minimize the number of retrieved raw vectors from disks.</p> <ol> <li>Sort the candidates with respect to their estimated distances.</li> <li>Initialize a max-heap (sorted by upper bounds of distances) <code>KNNs</code> with the top-K candidates which have the smallest estimated distances.</li> <li>Enumerate the remaining candidates. For a new candidate A, <ol> <li>[Condition 1 - A's lower bound &gt; the maximum upper bound in <code>KNNs</code>.] <ol> <li>Drop the new candidate and move on. </li> </ol> </li> <li>[Condition 2 -  A's lower bound \\le the maximum upper bound in <code>KNNs</code>, and the top candidate in <code>KNNs</code> has been reranked.]<ol> <li>Rerank the new candidate, update <code>KNNs</code> and move on.</li> </ol> </li> <li>[Condition 3 - A's lower bound \\le the maximum upper bound in <code>KNNs</code>, and the top candidate in <code>KNNs</code> has NOT been reranked.]<ol> <li>Rerank the top candidate in <code>KNNs</code>, update <code>KNNs</code> and repeat the procedure for candidate A. </li> </ol> </li> </ol> </li> </ol>"},{"location":"rabitq/rotator/","title":"Random Rotation","text":"<p>Random rotation (i.e., Johnson Lindenstrauss Transformation) is a crucial step to ensure robust performance and theoretical error bounds of RaBitQ. It is applied to all vectors (including raw data vectors, center vectors and raw query vectors) as a preprocessing step. This section describes the usage of the random rotation.</p> <p>RaBitQLib provides two types of random rotation. All implementations sample and store a random rotation at first. Then they apply the sampled random rotation to every input vector and return the rotated vector. </p> <p>By default, the library uses the <code>FFHT + Kac\u2019s Walk</code> method. </p> <p>The implementation can be found in <code>rotator.hpp</code>.</p> <pre><code>.\n\u251c\u2500\u2500 rabitqlib\n\u2502   \u251c\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 utils\n\u2502       \u251c\u2500\u2500 ...\n\u2502       \u2514\u2500\u2500 rotator.hpp\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"rabitq/rotator/#example","title":"Example","text":"<pre><code>// Initialize a rotator\n// Version 1 - the default rotator\n// vectors are padded to the smallest multiple of 64\n// storage - 4D bits, time - O(D * log D)\nrabitqlib::Rotator&lt;float&gt;* rotator = rabitqlib::choose_rotator&lt;float&gt;(\n    dim = dim, \n    RotatorType type = RotatorType::FhtKacRotator);\n\n// Initialize a rotator\n// Version 2 - the random orthogonal transformation\n// vectors are padded to the smallest multiple of 64\n// storage - D * D floats, time - O(D * D)\nrabitqlib::Rotator&lt;float&gt;* rotator = rabitqlib::choose_rotator&lt;float&gt;(\n    dim = dim, \n    RotatorType type = RotatorType::MatrixRotator);\n\n// Apply a rotator to a vector\nsize_t dim = 768;\nstd::vector&lt;float&gt; x(dim);\nstd::vector&lt;float&gt; x_prime(dim);\n... \nrotator -&gt; rotate(x.data(), x_prime.data())\n</code></pre>"},{"location":"rabitq/rotator/#ffht-kacs-walk","title":"FFHT + Kac\u2019s Walk","text":""},{"location":"rabitq/rotator/#description","title":"Description","text":"<p>This method is a combination of the well-known Fast Johnson-Lindenstrauss Transformation algorithms based on Fast Hadamard Transform and ideas in Kac\u2019s Walk.  It first samples 4 sequences of random signs (i.e., Rademacher random variables). Then for each vector, it repeats the following procedures 4 times.</p> <ol> <li>Flip its coordinates with the i-th sequence of sampled random signs. </li> <li>Apply FFHT on the first/last 2^k coordinates (alternately), where 2^k is the maximum power of 2 that is less than or equal to the dimensionality of the vector.</li> <li>Apply Givens rotation with a fixed angle \\theta = \\frac{\\pi}{4} to the 1st and the 2nd halves of coordinates.</li> </ol> <p>The following table summarizes the space and time complexity of this method.</p> Space Consumption Time Complexity 4D binary values (4D bits) O(D\\log D) <p>This implementation is based on the FFHT library developed by Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn and Ludwig Schmidt. </p>"},{"location":"rabitq/rotator/#random-orthogonal-transformation","title":"Random Orthogonal Transformation","text":""},{"location":"rabitq/rotator/#description_1","title":"Description","text":"<p>This method is the classical Johnson-Lindenstrauss Transformation. It first samples a random gaussian matrix and orthogonalizes it with QR decomposition. Then it multiplies the matrix to every vector.</p> <p>The following table summarizes the space and time complexity of this method.</p> Space Consumption Time Complexity D^2 floating-point numbers O(D^2)"}]}